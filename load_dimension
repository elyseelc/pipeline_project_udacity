from airflow.hooks.postgres_hook import PostgresHook
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
from airflow.secrets.metastore import MetastoreBackend
from helpers.sql_queries import SqlQueries

class LoadDimensionOperator(BaseOperator):
    """A custom operator that transfers data from staging tables to Redshift dimension tables.

    Class variables:
    ui_color: Sets a color for the Airflow DAG task in the user interface.

    Dependencies:
    The dimension table must have been previously created in Redshift.
    """
    ui_color = '#80BD9E'

    @apply_defaults
    def __init__(self,
                 redshift_conn_id="",
                 aws_credentials_id="",
                 dim_table="",
                 select_sql="",
                 append_only=False,
                 *args, **kwargs):
        """Initialise an instance of the LoadDimensionOperator class.

        Parameters
        redshift_conn_id: The Airflow connection ID for Redshift.
        aws_credentials_id: The Airflow connection ID for AWS credentials.
        dim_table: The name of the dimension table.
        select_sql: The SQL query to select and move data from the staging area.

        These parameters are passed when an instance of the class is created, 
        which would typically be a task in the main DAG.
        """

        super(LoadDimensionOperator, self).__init__(*args, **kwargs)
        self.redshift_conn_id = redshift_conn_id
        self.aws_credentials_id = aws_credentials_id
        self.dim_table = dim_table
        self.select_sql = select_sql
        self.append_only = append_only

    def execute(self, context):
        """Establishes connections with Redshift and executes the specified SQL query.

        Upon the creation of an instance of this class in the main DAG script,
        a connection to Redshift is established. The attrs of the select_sql are retrieved 
        and data is inserted into the dim_table.
        """

        self.log.info('LoadDimensionOperator instance created')

        metastoreBackend = MetastoreBackend()
        aws_connection=metastoreBackend.get_connection(self.aws_credentials_id)

        redshift_hook = PostgresHook(postgres_conn_id=self.redshift_conn_id)

        if not self.append_only:
            self.log.info(f"Clearing data from destination Redshift table {self.dim_table}")
            redshift_hook.run(f"DELETE FROM {self.dim_table}")

        self.log.info(f"Running data load into {self.dim_table}")
        formatted_sql = getattr(SqlQueries, self.select_sql).format(self.dim_table)

        redshift_hook.run(formatted_sql)
